{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from datetime import datetime\n",
    "\n",
    "#this code below is to make %%time magic work\n",
    "import glob\n",
    "import os\n",
    "from __future__ import print_function\n",
    "#this code above is to make %%time magic work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101766 entries, 0 to 101765\n",
      "Columns: 245 entries, meds_increased to readmitted_tf\n",
      "dtypes: int64(245)\n",
      "memory usage: 190.2 MB\n"
     ]
    }
   ],
   "source": [
    "#import the data\n",
    "directory = 'C:/Users/N1110/Desktop/7331_Project/data/'\n",
    "df = pd.read_csv(directory + 'Diabetes_Imputed_Cleaned.csv')\n",
    "df_imputed = df\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'readmitted_tf' in df_imputed:\n",
    "    y = df_imputed['readmitted_tf'].values # get the labels we want\n",
    "    del df_imputed['readmitted_tf'] # get rid of the class label\n",
    "    X = df_imputed.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\N1110\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\N1110\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\N1110\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\N1110\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "## Training and Testing Split\n",
    "# okay, so run through the cross validation loop and set the training and testing variable for one single iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "        # we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scl_obj = StandardScaler()\n",
    "\n",
    "scl_obj.fit(X_train)\n",
    "X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How XGBoost work\n",
    "#### boosting\n",
    "A ML technique that iteratively combines a set of simple and not very accurate classifiers (referred to as \"weak\" classifiers) into a classifier with high accuracy (a \"strong\" classifier) by upweighting the examples that the model is currently misclassifying [3].\n",
    "\n",
    "As opposed to the bagging where trees are built parallelly, in boosting, the trees are built sequentially such that each subsequent tree aims to reduce the errors of the previous tree. Each tree learns from its predecessors and updates the residual errors. Hence, the tree that grows next in the sequence will learn from an updated version of the residuals [2].  \n",
    "1.\tFit a model to the data, F1(X)=Y\n",
    "2.\tFit a model to the residuals, h1(X) = Y - F1(X)\n",
    "3.\tCreate a new model, F2(X) = F1(X) + h1(X) [Note: F2 is boosted version of F1]\n",
    "and goes on… Fm(X) = F(m-1)(X) + h(m-1)(X)\n",
    "\n",
    "#### gradient descent\n",
    "A technique to minimize loss by computing the gradients of loss with respect to the model's parameters, conditioned on training data. Informally, gradient descent iteratively adjusts parameters, gradually finding the best combination of weights and bias to minimize loss [3].\n",
    "\n",
    "#### Gradient Boosting\n",
    "Gradient Boosting employs the gradient descent algorithm to minimize errors in sequential models. The major inefficiency in Gradient Boosting is that it creates one decision tree at a time.\n",
    "To overcome this, Tianqi Chen and Carlos Guestrin built A Scalable Tree Boosting System — XGBoost can be thought of as Gradient Boosting on steroids. It features parallelized tree building, cache-aware access, sparsity awareness, regularization, and\n",
    "weighted quantile sketch as some of its systems optimization and algorithmic enhancements [8].\n",
    "\n",
    "#### What is the difference between gradient boosting and adaboost?\n",
    "They differ on how they create the weak learners during the iterative process.\n",
    "At each iteration, adaptive boosting changes the sample distribution by modifying the weights attached to each of the instances. \n",
    "Gradient boosting doesn’t modify the sample distribution. Instead of training on a newly sample distribution, the weak learner trains on the remaining errors (so-called pseudo-residuals) of the strong learner [6].\n",
    "\n",
    "#### learning rate\n",
    "\n",
    "A scalar used to train a model via gradient descent. During each iteration, the gradient descent algorithm multiplies the learning rate by the gradient. The resulting product is called the gradient step.\n",
    "Learning rate is a key hyperparameter [3].\n",
    "\n",
    "\n",
    "#### What is XGBoost?\n",
    "XGBoost is short for eXtreme gradient boosting. It is a library designed and optimized for boosted tree algorithms. XGBoost is a state-of-the-art a scalable end-to-end tree boosting system, and also have a novel sparsity-aware algorithm for sparse data [1]. It is a proven model in data science competition and hackathons for its accuracy, speed and scale [2].  For classification problems, XGBoost yield the similar or better accuracy with sk-learn but 10 x faster runtime [1].\n",
    "\n",
    "In XGBoost, we fit a model on the gradient of loss generated from the previous step. In XGBoost, we just modified our gradient boosting algorithm so that it works with any differentiable loss function [2].  \n",
    "\n",
    "#### How should I interpret XGBoost trees?\n",
    "You can interpret xgboost model by interpreting individual trees [7].\n",
    "\n",
    "\n",
    "## Advantages of XGBoost\n",
    "#### Why does XGBoost perform better than SVM? \n",
    "In our project, we find SVM runs significantly longer than RF and XGBoost models, about 20 mins vs 2 mins vs 1 min. Also, the accuracy we got for these three models are:  XGBoost has around 0.75 auc while RF and SVM have similar auc of around 0.67 (see the comparison plot). RF actually has slightly higher auc than SVM.\n",
    "XGBoost is an ensamble method it uses many trees to take a decision so it gains power by repeating itself. SVM is a linear separator, when data is not linearly separable SVM needs a Kernel to project the data into a space where it can separate it, there lies its greatest strength and weakness, by being able to project data into a high dimensional space SVM can find a linear separation for almost any data but at the same time it needs to use a Kernel and we can argue that there's not a perfect kernel for every dataset [4].\n",
    "\n",
    "#### What makes xgboost run much faster than many other implementations of gradient boosting?\n",
    "XGBoost is designed for scale in as its initial goal. Memory optimization, Cache-line optimization, The improvement in terms of model itself, distributed version; push the limit of the tool to fully utilize the resources, so you can run more examples on single node; There is an external memory version, that allows you to store data in disk, without loading all of them in memory [5].\n",
    "\n",
    "Refs:\n",
    "1.\tChen et al 2016. XGBoost: A Scalable Tree Boosting System https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf\n",
    "\n",
    "2.\thttps://medium.com/@pushkarmandot/how-exactly-xgboost-works-a320d9b8aeef\n",
    "\n",
    "3.\thttps://developers.google.com/machine-learning/glossary\n",
    "4.\thttps://www.quora.com/  Why does XGBoost perform better than SVM?\n",
    "5.\thttps://www.quora.com/  What makes xgboost run much faster than many other implementations of gradient boosting?\n",
    "6.\thttps://www.quora.com/  What is the difference between gradient boosting and adaboost?\n",
    "7.\thttps://www.quora.com/ How should I interpret XGBoost trees?\n",
    "8.  https://media.licdn.com/dms/document/C4D1FAQF_5vlXos139Q/feedshare-document-pdf-analyzed/0?e=1561834800&v=beta&t=GuChwPbOqD4OZz3vKAvfF81lRmtycXS6dooNNHeafTw From Zero to Hero in XGBoost Tuning\n",
    "\n",
    "XGBoost Python Package\n",
    "https://xgboost.readthedocs.io/en/latest/python/ \n",
    "https://github.com/dmlc/xgboost/tree/master/python-package\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.630101\tvalid-auc:0.626937\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 50 rounds.\n",
      "[50]\ttrain-auc:0.71629\tvalid-auc:0.714845\n",
      "[100]\ttrain-auc:0.724855\tvalid-auc:0.722444\n",
      "[150]\ttrain-auc:0.730268\tvalid-auc:0.727276\n",
      "[200]\ttrain-auc:0.73447\tvalid-auc:0.731385\n",
      "[250]\ttrain-auc:0.737574\tvalid-auc:0.734527\n",
      "[300]\ttrain-auc:0.74058\tvalid-auc:0.737548\n",
      "[350]\ttrain-auc:0.742633\tvalid-auc:0.739162\n",
      "[400]\ttrain-auc:0.744783\tvalid-auc:0.741004\n",
      "[450]\ttrain-auc:0.746347\tvalid-auc:0.742047\n",
      "[500]\ttrain-auc:0.747737\tvalid-auc:0.743198\n",
      "[550]\ttrain-auc:0.749098\tvalid-auc:0.74403\n",
      "[600]\ttrain-auc:0.750151\tvalid-auc:0.744622\n",
      "[650]\ttrain-auc:0.751088\tvalid-auc:0.745281\n",
      "[700]\ttrain-auc:0.752051\tvalid-auc:0.745989\n",
      "[750]\ttrain-auc:0.75279\tvalid-auc:0.746322\n",
      "[800]\ttrain-auc:0.753532\tvalid-auc:0.746853\n",
      "[850]\ttrain-auc:0.75442\tvalid-auc:0.747438\n",
      "[900]\ttrain-auc:0.755043\tvalid-auc:0.747819\n",
      "[950]\ttrain-auc:0.755691\tvalid-auc:0.748193\n",
      "[999]\ttrain-auc:0.756217\tvalid-auc:0.748447\n"
     ]
    }
   ],
   "source": [
    "# XGBoost baseline model\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "#XGBOOST parameters 1\n",
    "MAX_ROUNDS = 1000\n",
    "EARLY_STOP = 50\n",
    "OPT_ROUNDS = 1000\n",
    "VERBOSE_EVAL = 50\n",
    "RANDOM_STATE = 2000\n",
    "\n",
    "#XGBOOST transform data into DMatrix format for modeling\n",
    "dtrain = xgb.DMatrix(X_train_scaled, y_train)\n",
    "dvalid = xgb.DMatrix(X_test_scaled, y_test)\n",
    "type(dtrain)\n",
    "\n",
    "\n",
    "# XGBoost Parameters 2\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "#params['objective'] = 'multi:softmax'\n",
    "#params['objective'] = 'reg:linear'\n",
    "params['eta'] = 0.039\n",
    "params['silent'] = True\n",
    "params['max_depth'] = 2\n",
    "params['subsample'] = 0.8\n",
    "params['colsample_bytree'] = 0.9\n",
    "params['eval_metric'] = 'auc'\n",
    "params['random_state'] = RANDOM_STATE\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.630101\tvalid-auc:0.626937\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 50 rounds.\n",
      "[50]\ttrain-auc:0.71629\tvalid-auc:0.714845\n",
      "[100]\ttrain-auc:0.724855\tvalid-auc:0.722444\n",
      "[150]\ttrain-auc:0.730268\tvalid-auc:0.727276\n",
      "[200]\ttrain-auc:0.73447\tvalid-auc:0.731385\n",
      "[250]\ttrain-auc:0.737574\tvalid-auc:0.734527\n",
      "[300]\ttrain-auc:0.74058\tvalid-auc:0.737548\n",
      "[350]\ttrain-auc:0.742633\tvalid-auc:0.739162\n",
      "[400]\ttrain-auc:0.744783\tvalid-auc:0.741004\n",
      "[450]\ttrain-auc:0.746347\tvalid-auc:0.742047\n",
      "[500]\ttrain-auc:0.747737\tvalid-auc:0.743198\n",
      "[550]\ttrain-auc:0.749098\tvalid-auc:0.74403\n",
      "[600]\ttrain-auc:0.750151\tvalid-auc:0.744622\n",
      "[650]\ttrain-auc:0.751088\tvalid-auc:0.745281\n",
      "[700]\ttrain-auc:0.752051\tvalid-auc:0.745989\n",
      "[750]\ttrain-auc:0.75279\tvalid-auc:0.746322\n",
      "[800]\ttrain-auc:0.753532\tvalid-auc:0.746853\n",
      "[850]\ttrain-auc:0.75442\tvalid-auc:0.747438\n",
      "[900]\ttrain-auc:0.755043\tvalid-auc:0.747819\n",
      "[950]\ttrain-auc:0.755691\tvalid-auc:0.748193\n",
      "[999]\ttrain-auc:0.756217\tvalid-auc:0.748447\n",
      "Wall time: 3min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_clf = xgb.train(params,\n",
    "                   dtrain,\n",
    "                   MAX_ROUNDS,\n",
    "                   watchlist,\n",
    "                   early_stopping_rounds = EARLY_STOP,\n",
    "                   maximize = True,\n",
    "                   verbose_eval = VERBOSE_EVAL)\n",
    "\n",
    "\n",
    "#Wall time: 3min 48s\n",
    "#[999]\ttrain-auc:0.756217\tvalid-auc:0.748447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - roc_auc_score:  0.7484469077761282\n"
     ]
    }
   ],
   "source": [
    "preds = xgb_clf.predict(dvalid)\n",
    "print('XGBoost - roc_auc_score: ', roc_auc_score(y_test, preds))\n",
    "\n",
    "#XGBoost - roc_auc_score:  0.7484469077761282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - XGBoost\n",
    "\n",
    "https://www.kaggle.com/tilii7/hyperparameter-grid-search-with-xgboost\n",
    "\n",
    "We used both randomized search and grid search for RF models for comparsion purpose. To save time, we only run randomized search for XGBoost.\n",
    "\n",
    "In addition, due to the limited computation power on my personal computer, I only run on 3-fold validation and limited parameter sampling for a reasonable runtime like 0.5-1 hr. Otherwise, it will take very long time like 5-10 hrs to run. Therefore, this search does not produce a great score (actually worse than baseline model).\n",
    "\n",
    "Subsampling of 20% of our data also serves the purpose of save runtime. Plus, we found the results is similar with full dataset run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4071 entries, 19298 to 74072\n",
      "Columns: 244 entries, meds_increased to age\n",
      "dtypes: int64(244)\n",
      "memory usage: 7.6 MB\n"
     ]
    }
   ],
   "source": [
    "#for the sake of time, subsample 20% of our data \n",
    "\n",
    "subsam_df_imputed=df_imputed.sample(frac=0.2)\n",
    "subsam_df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4071 entries, 19298 to 74072\n",
      "Columns: 244 entries, meds_increased to age\n",
      "dtypes: int64(244)\n",
      "memory usage: 7.6 MB\n"
     ]
    }
   ],
   "source": [
    "df_imputed=subsam_df_imputed\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'readmitted_tf' in df_imputed:\n",
    "    y = df_imputed['readmitted_tf'].values # get the labels we want\n",
    "    del df_imputed['readmitted_tf'] # get rid of the class label\n",
    "    X = df_imputed.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\N1110\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\N1110\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\N1110\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\N1110\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "## Training and Testing Split\n",
    "# okay, so run through the cross validation loop and set the training and testing variable for one single iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    # we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scl_obj = StandardScaler()\n",
    "\n",
    "scl_obj.fit(X_train)\n",
    "X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A parameter grid for XGBoost\n",
    "#at least include the parameters already included in the baseline model for chance to find one better than base model.\n",
    "params = {\n",
    "        'min_child_weight': [1, 5],\n",
    "        'gamma': [0.5, 1, 2],\n",
    "        'subsample': [0.6, 0.8],\n",
    "        'colsample_bytree': [0.6, 0.9],\n",
    "        'max_depth': [2, 5]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### watch out for total number of created models\n",
    "difference combination of the features. Altogether, there are 2*2*2*2*2=32 settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n",
    "                    silent=True, nthread=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems XGBClassifier does not require to transform into DMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search in a parallized fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=4)]: Done   2 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   9 | elapsed: 10.6min remaining: 21.2min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   9 | elapsed: 10.6min remaining: 13.3min\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   9 | elapsed: 11.8min remaining:  9.4min\n",
      "[Parallel(n_jobs=4)]: Done   6 out of   9 | elapsed: 17.2min remaining:  8.6min\n",
      "[Parallel(n_jobs=4)]: Done   7 out of   9 | elapsed: 17.2min remaining:  4.9min\n",
      "[Parallel(n_jobs=4)]: Done   9 out of   9 | elapsed: 17.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   9 out of   9 | elapsed: 17.9min finished\n",
      "\n",
      " Time taken: 0 hours 24 minutes and 52.36 seconds.\n"
     ]
    }
   ],
   "source": [
    "# estimate runtime 40 mins\n",
    "# actual runtime 30 mins\n",
    "## grid search in a parallized fashion\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from datetime import datetime\n",
    "\n",
    "folds = 3\n",
    "param_comb = 3\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train_scaled,y_train), verbose=50, random_state=2000)\n",
    "\n",
    "# Here we go\n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best estimator:\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=0.6, gamma=2, learning_rate=0.02, max_delta_step=0,\n",
      "       max_depth=5, min_child_weight=1, missing=None, n_estimators=600,\n",
      "       n_jobs=1, nthread=1, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.6)\n",
      "\n",
      " Best normalized gini score for 3-fold search with 3 parameter combinations:\n",
      "0.5110818135734807\n",
      "\n",
      " Best hyperparameters:\n",
      "{'subsample': 0.6, 'min_child_weight': 1, 'max_depth': 5, 'gamma': 2, 'colsample_bytree': 0.6}\n"
     ]
    }
   ],
   "source": [
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)\n",
    "print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(random_search.best_score_ * 2 - 1)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results - Best hyperparameters\n",
    "Best estimator:\n",
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.6, gamma=2, learning_rate=0.02, max_delta_step=0,\n",
    "       max_depth=5, min_child_weight=1, missing=None, n_estimators=600,\n",
    "       n_jobs=1, nthread=1, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=0.6)\n",
    "\n",
    " Best normalized gini score for 3-fold search with 3 parameter combinations:\n",
    "0.5110818135734807\n",
    "\n",
    " Best hyperparameters:\n",
    "{'subsample': 0.6, 'min_child_weight': 1, 'max_depth': 5, 'gamma': 2, 'colsample_bytree': 0.6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Time taken: 0 hours 8 minutes and 24.05 seconds.\n"
     ]
    }
   ],
   "source": [
    "#run for \n",
    "#evaulate the performance of the model best parameters\n",
    "# timer the training time\n",
    "start_time = timer(None)\n",
    "\n",
    "xgb_best = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.6, gamma=2, learning_rate=0.02, max_delta_step=0,\n",
    "       max_depth=5, min_child_weight=1, missing=None, n_estimators=600,\n",
    "       n_jobs=1, nthread=1, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=0.6)\n",
    "\n",
    "xgb_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "timer(start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Time taken: 0 hours 0 minutes and 0.91 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\N1110\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# timer the testing time\n",
    "start_time = timer(None)\n",
    "preds = xgb_best.predict(X_test_scaled)\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB - roc_auc_score:  0.677932172132421\n"
     ]
    }
   ],
   "source": [
    "print('XGB - roc_auc_score: ', roc_auc_score(y_test, preds)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement of -9.42%.\n"
     ]
    }
   ],
   "source": [
    "# The reason I want to keep both baseline model and refined model is to show the effect of Hyperparameter Tuning. \n",
    "# To see increase the AUC by what percentage.\n",
    "\n",
    "XGBrandom_accuracy = 0.677932172132421\n",
    "XGBbase_accuracy =0.748447\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (XGBrandom_accuracy - XGBbase_accuracy) / XGBbase_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not surprisingly, this search does not produce a great score because of 3-fold validation and limited parameter sampling. The randomized grid search is only for demo purpose. We should provide a larger list of parameters in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
